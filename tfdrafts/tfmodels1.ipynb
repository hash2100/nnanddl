{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2f6020b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: [672545.63]\n",
      "loss: [482903.0]\n",
      "loss: [475800.97]\n",
      "loss: [470492.38]\n",
      "loss: [466524.13]\n",
      "loss: [463557.84]\n",
      "loss: [461340.41]\n",
      "loss: [459682.78]\n",
      "loss: [458443.47]\n",
      "loss: [457516.94]\n",
      "loss: [456824.16]\n",
      "loss: [456306.13]\n",
      "loss: [455918.66]\n",
      "loss: [455628.75]\n",
      "loss: [455411.91]\n",
      "loss: [455249.56]\n",
      "loss: [455127.94]\n",
      "loss: [455036.81]\n",
      "loss: [454968.5]\n",
      "loss: [454917.16]\n",
      "loss: [454878.53]\n",
      "loss: [454849.44]\n",
      "loss: [454827.44]\n",
      "loss: [454810.75]\n",
      "loss: [454798.09]\n",
      "loss: [454788.34]\n",
      "loss: [454780.81]\n",
      "loss: [454774.97]\n",
      "loss: [454770.38]\n",
      "loss: [454766.66]\n",
      "loss: [454763.69]\n",
      "loss: [454761.19]\n",
      "loss: [454759.06]\n",
      "loss: [454757.28]\n",
      "loss: [454755.72]\n",
      "loss: [454754.28]\n",
      "loss: [454752.97]\n",
      "loss: [454751.78]\n",
      "loss: [454750.59]\n",
      "loss: [454749.53]\n",
      "loss: [454748.47]\n",
      "loss: [454747.47]\n",
      "loss: [454746.41]\n",
      "loss: [454745.41]\n",
      "loss: [454744.41]\n",
      "loss: [454743.44]\n",
      "loss: [454742.53]\n",
      "loss: [454741.59]\n",
      "loss: [454740.63]\n",
      "loss: [454739.66]\n",
      "loss: [454738.69]\n",
      "loss: [454737.78]\n",
      "loss: [454736.81]\n",
      "loss: [454735.81]\n",
      "loss: [454734.88]\n",
      "loss: [454733.94]\n",
      "loss: [454732.97]\n",
      "loss: [454732.06]\n",
      "loss: [454731.13]\n",
      "loss: [454730.19]\n",
      "loss: [454729.19]\n",
      "loss: [454728.25]\n",
      "loss: [454727.31]\n",
      "loss: [454726.38]\n",
      "loss: [454725.41]\n",
      "loss: [454724.44]\n",
      "loss: [454723.56]\n",
      "loss: [454722.59]\n",
      "loss: [454721.63]\n",
      "loss: [454720.69]\n",
      "loss: [454719.75]\n",
      "loss: [454718.78]\n",
      "loss: [454717.84]\n",
      "loss: [454716.88]\n",
      "loss: [454715.97]\n",
      "loss: [454715.0]\n",
      "loss: [454714.06]\n",
      "loss: [454713.09]\n",
      "loss: [454712.13]\n",
      "loss: [454711.22]\n",
      "loss: [454710.22]\n",
      "loss: [454709.31]\n",
      "loss: [454708.31]\n",
      "loss: [454707.41]\n",
      "loss: [454706.47]\n",
      "loss: [454705.47]\n",
      "loss: [454704.56]\n",
      "loss: [454703.63]\n",
      "loss: [454702.72]\n",
      "loss: [454701.75]\n",
      "loss: [454700.75]\n",
      "loss: [454699.84]\n",
      "loss: [454698.84]\n",
      "loss: [454697.91]\n",
      "loss: [454697.0]\n",
      "loss: [454696.06]\n",
      "loss: [454695.09]\n",
      "loss: [454694.16]\n",
      "loss: [454693.19]\n",
      "loss: [454692.28]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# initialize model parameters\n",
    "W = tf.Variable(tf.zeros([2, 1]), name=\"weights\")\n",
    "#W = tf.Variable(tf.zeros([5, 1]), name=\"weights\")\n",
    "b = tf.Variable(0., name=\"bias\")\n",
    "lmbda = tf.constant(0.5)\n",
    "\n",
    "def inference(X):\n",
    "    return tf.matmul(X, W) + b\n",
    "\n",
    "def loss(X, Y, sess):\n",
    "    Y_predicted = inference(X)\n",
    "    return tf.add(\n",
    "        tf.reduce_sum(tf.squared_difference(Y, Y_predicted)),\n",
    "        tf.multiply(lmbda, tf.tensordot(W, W, [0, 0])[0,0])\n",
    "    )\n",
    "    #return tf.reduce_sum(tf.squared_difference(Y, Y_predicted))\n",
    "\n",
    "def inputs():\n",
    "    weight_age = [[84, 46], [73, 20], [65, 52], [70, 30], [76, 57], [69, 25], [63, 28], [72, 36]]\n",
    "    #weight_age = np.array([ [x[0], x[1], x[0]**2, x[1]**2, x[0]*x[1] ] for x in weight_age])\n",
    "    blood_fat_content = [354, 190, 405, 263, 451, 302, 288, 385]\n",
    "    \n",
    "    return tf.to_float(weight_age), tf.to_float(blood_fat_content)\n",
    "\n",
    "def train(total_loss):\n",
    "    learning_rate = 1E-6\n",
    "    return tf.train.GradientDescentOptimizer(learning_rate).minimize(total_loss)\n",
    "\n",
    "def evaluate(sess, X, Y):\n",
    "    #print(sess.run(inference([[80., 25.]]))) # 368\n",
    "    #print(sess.run(inference([[65., 25.]]))) # 298\n",
    "    pass\n",
    "    \n",
    "    \n",
    "# launch the graph in a session\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    X, Y = inputs()    \n",
    "    out = sess.run([X, Y])\n",
    "    weight = [x[0] for x in out[0]]\n",
    "    age = [x[1] for x in out[0]]\n",
    "    fats = out[1]\n",
    "    plt.scatter(fats, weight)\n",
    "    plt.show()\n",
    "\n",
    "    total_loss = loss(X, Y, sess)\n",
    "    train_op = train(total_loss)\n",
    "    \n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "    \n",
    "    # actual training loop\n",
    "    training_steps = 1000\n",
    "    for step in range(training_steps):\n",
    "        sess.run([train_op])\n",
    "        \n",
    "        #print(\"W: %s\" % sess.run(W))\n",
    "        #print(\"W_norm: %s\" % sess.run(tf.norm(W)))\n",
    "        #print(\"W_squared: %s\" % sess.run(tf.tensordot(W, W, [0, 0])[0,0]))\n",
    "        \n",
    "        # for debugging and learning purposes, see how the loss gets decremented thru training steps\n",
    "        if step % 10 == 0:\n",
    "            print(\"loss: %s\" % sess.run([total_loss]))\n",
    "            \n",
    "    evaluate(sess, X, Y)\n",
    "    \n",
    "    coord.request_stop()\n",
    "    coord.join(threads)\n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression for Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# read the Iris data\n",
    "irisfile = open('iris.data.csv')\n",
    "reader = csv.reader(irisfile, delimiter=',')\n",
    "iris_data = [x for x in reader]\n",
    "irisfile.close()\n",
    "\n",
    "# take away the header and a straggle '\\n'\n",
    "iris_header = iris_data[0]\n",
    "iris_data = iris_data[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: [1.0861624]\n",
      "loss: [0.35761866]\n",
      "loss: [0.27048293]\n",
      "loss: [0.22409287]\n",
      "loss: [0.19499224]\n",
      "loss: [0.17496037]\n",
      "loss: [0.16027805]\n",
      "loss: [0.14901851]\n",
      "loss: [0.14008458]\n",
      "loss: [0.13280535]\n",
      "0.986667\n"
     ]
    }
   ],
   "source": [
    "# since there are 4 features and 3 output classes, the weights would have this dimension\n",
    "W = tf.Variable(tf.zeros([4, 3]), name=\"weights\")\n",
    "b = tf.Variable(tf.zeros([3]), name=\"bias\")\n",
    "\n",
    "def combine_inputs(X):\n",
    "    return tf.matmul(X, W) + b\n",
    "\n",
    "def inference(X):\n",
    "    return tf.nn.softmax(combine_inputs(X))\n",
    "\n",
    "def loss(X, Y):\n",
    "    return tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=combine_inputs(X), labels=Y))\n",
    "\n",
    "def inputs(iris_data):\n",
    "    l_setosa = [ int(row == 'Iris-setosa') for row in [x[-1] for x in iris_data] ]\n",
    "    l_versicolor = [ int(row == 'Iris-versicolor') for row in [x[-1] for x in iris_data] ]\n",
    "    l_virginica = [ int(row == 'Iris-virginica') for row in [x[-1] for x in iris_data] ]\n",
    "    \n",
    "    label_number = tf.to_int32(np.argmax([l_setosa, l_versicolor, l_virginica], 0))\n",
    "    features = tf.to_float([ [float(y) for y in x[:-1]] for x in iris_data ])\n",
    "    \n",
    "    return features, label_number\n",
    "\n",
    "def train(total_loss):\n",
    "    learning_rate = 1E-2\n",
    "    return tf.train.GradientDescentOptimizer(learning_rate).minimize(total_loss)\n",
    "    \n",
    "def evaluate(sess, X, Y):\n",
    "    predicted = tf.cast(tf.argmax(inference(X), 1), tf.int32)\n",
    "    \n",
    "    print(sess.run(tf.reduce_mean(tf.cast(tf.equal(predicted, Y), tf.float32))))\n",
    "    \n",
    "    \n",
    "# launch the graph in a session\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    X, Y = inputs(iris_data)\n",
    "    \n",
    "    total_loss = loss(X, Y)\n",
    "    train_op = train(total_loss)\n",
    "    \n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "    \n",
    "    # actual training loop\n",
    "    training_steps = 10000\n",
    "    for step in range(training_steps):\n",
    "        sess.run([train_op])\n",
    "        \n",
    "        # for debugging and learning purposes, see how the loss gets decremented thru training steps\n",
    "        if step % 1000 == 0:\n",
    "            print(\"loss: %s\" % sess.run([total_loss]))\n",
    "            \n",
    "    evaluate(sess, X, Y)\n",
    "    \n",
    "    coord.request_stop()\n",
    "    coord.join(threads)\n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutions an kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ 11.]\n",
      "   [ 10.]]\n",
      "\n",
      "  [[  5.]\n",
      "   [  4.]]]\n",
      "\n",
      "\n",
      " [[[ 34.]\n",
      "   [ 26.]]\n",
      "\n",
      "  [[ 14.]\n",
      "   [ 10.]]]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# shape: images x height x width x channels per pixel \n",
    "input_batch = tf.constant([\n",
    "    [ # First image, every pixel has only one channel\n",
    "        [[0.0], [1.0], [2.0]],\n",
    "        [[2.0], [3.0], [4.0]]\n",
    "    ],\n",
    "    [ # Second image\n",
    "        [[2.0], [4.0], [6.0]],\n",
    "        [[6.0], [8.0], [10.0]]\n",
    "    ]\n",
    "])\n",
    "\n",
    "# kernel dimension: height x width x in_channels x out_channels\n",
    "kernel = tf.constant([\n",
    "    [[[1.0]], [[1.0]]],\n",
    "    [[[2.0]], [[2.0]]]\n",
    "])\n",
    "\n",
    "# only the middle strides are used; 1 x 2 in the middle means columns get decimated\n",
    "conv2d = tf.nn.conv2d(input_batch, kernel, strides=[1, 1, 2, 1], padding='SAME')\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    print(sess.run(conv2d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([-0.1,  0. ,  0.1,  0.2], dtype=float32), array([-0. ,  0. ,  0.2,  0. ], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    features = tf.constant([-0.1, 0.0, 0.1, 0.2])\n",
    "    print(sess.run([features, tf.nn.dropout(features, keep_prob=0.5)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[[[ 1.]],\n",
      "\n",
      "        [[ 2.]],\n",
      "\n",
      "        [[ 3.]]]], dtype=float32), array([[[[ 0.70710677]],\n",
      "\n",
      "        [[ 0.89442718]],\n",
      "\n",
      "        [[ 0.94868326]]]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    layer_input = tf.constant([\n",
    "        [ [[ 1.]], [[ 2.]], [[ 3.]] ]\n",
    "    ])\n",
    "    lrn = tf.nn.local_response_normalization(layer_input)\n",
    "    print(sess.run([layer_input, lrn]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7071067811865475, 0.8944271909999159, 0.9486832980505138]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math as m\n",
    "v = [1.0, 2.0, 3.0]\n",
    "[x / m.sqrt(1 + x**2) for x in v]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words (+UNK) [['UNK', 1], (',', 29009), ('the', 16546), ('.', 15331), ('and', 8818)]\n",
      "Sample data [31, 249, 250, 4732, 6, 1519, 1520, 2120, 1, 39] ['The', 'Project', 'Gutenberg', 'EBook', 'of', 'Ten', 'Years', 'Later', ',', 'by']\n",
      "249 Project -> 250 Gutenberg\n",
      "249 Project -> 31 The\n",
      "250 Gutenberg -> 249 Project\n",
      "250 Gutenberg -> 4732 EBook\n",
      "4732 EBook -> 6 of\n",
      "4732 EBook -> 250 Gutenberg\n",
      "6 of -> 1519 Ten\n",
      "6 of -> 4732 EBook\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import re\n",
    "import os\n",
    "import collections\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "\n",
    "TOKEN_REGEX = re.compile(r'[A-Za-z]+|[!?.:,()]')\n",
    "source_path = 'd:/projects/python/nnanddl/books'\n",
    "vocabulary_size = 0\n",
    "\n",
    "words = []\n",
    "for file in os.listdir(source_path):\n",
    "    with open(os.path.join(source_path, file), 'rb') as fin:\n",
    "        for line in fin:\n",
    "            words_on_line = TOKEN_REGEX.findall(line.decode().strip())\n",
    "            words.extend(words_on_line)\n",
    "\n",
    "    \n",
    "def build_dataset(words):\n",
    "    \"\"\"Process raw inputs into a dataset.\"\"\"\n",
    "    count = [['UNK', -1]]\n",
    "    counter = collections.Counter(words)\n",
    "    count.extend(counter.most_common(len(counter.keys()) - 1))\n",
    "    global vocabulary_size\n",
    "    vocabulary_size = len(counter.keys()) - 1\n",
    "        \n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "            \n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        index = dictionary.get(word, 0)\n",
    "        if index == 0:  # dictionary['UNK']\n",
    "            unk_count += 1\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, count, dictionary, reversed_dictionary\n",
    "\n",
    "\n",
    "data, count, dictionary, reverse_dictionary = build_dataset(words)\n",
    "print('Most common words (+UNK)', count[:5])\n",
    "print('Sample data', data[:10], [reverse_dictionary[i] for i in data[:10]])\n",
    "\n",
    "data_index = 0\n",
    "\n",
    "def generate_batch(batch_size, num_skips, skip_window):\n",
    "    global data_index\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = 2 * skip_window + 1  # [ skip_window target skip_window ]\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    if data_index + span > len(data):\n",
    "        data_index = 0\n",
    "    buffer.extend(data[data_index:data_index + span])\n",
    "    data_index += span\n",
    "    for i in range(batch_size // num_skips):\n",
    "        context_words = [w for w in range(span) if w != skip_window]\n",
    "        words_to_use = random.sample(context_words, num_skips)\n",
    "        for j, context_word in enumerate(words_to_use):\n",
    "            batch[i * num_skips + j] = buffer[skip_window]\n",
    "            labels[i * num_skips + j, 0] = buffer[context_word]\n",
    "        if data_index == len(data):\n",
    "            buffer.extend(data[:span])\n",
    "            data_index = span\n",
    "        else:\n",
    "            buffer.append(data[data_index])\n",
    "            data_index += 1\n",
    "    # Backtrack a little bit to avoid skipping words in the end of a batch\n",
    "    data_index = (data_index + len(data) - span) % len(data)\n",
    "    return batch, labels\n",
    "\n",
    "batch, labels = generate_batch(batch_size=8, num_skips=2, skip_window=1)\n",
    "for i in range(8):\n",
    "    print(batch[i], reverse_dictionary[batch[i]],\n",
    "        '->', labels[i, 0], reverse_dictionary[labels[i, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step  0 :  249.691711426\n",
      "Nearest to .: sextons, Godwin, idol, incomparable, hawk, curious, rests, cachet,\n",
      "Nearest to one: mid, damages, mood, Goring, halt, attentive, handwriting, declarations,\n",
      "Nearest to she: fork, Malta, onwards, fond, mutttered, pink, breathlessly, derogatory,\n",
      "Nearest to at: priceless, landlords, concerned, license, infected, DAMAGES, assign, mission,\n",
      "Nearest to your: prisons, dias, copied, appearances, Nearly, blood, intimidate, leaped,\n",
      "Nearest to M: coachman, amused, Jackdaws, depicting, WE, dee, unaffected, land,\n",
      "Nearest to to: copyright, wittiest, youth, argue, fretful, thank, swear, scarps,\n",
      "Nearest to had: Proof, statements, alighted, sheet, acutest, Internal, candidate, unconsciously,\n",
      "Nearest to What: tourmaline, oaths, impetuosity, thoroughly, NECKLACE, DIRECT, irresistibly, awful,\n",
      "Nearest to said: bane, reminds, swarming, descendants, professor, regretfully, bar, yawning,\n",
      "Nearest to very: manifestation, received, spectacula, outbuildings, murder, dresses, begins, attentive,\n",
      "Nearest to ,: marvellous, choice, prison, Rise, prowling, accompaniments, Fahrenheit, impetuously,\n",
      "Nearest to was: preposterous, facts, snap, EXCEPT, fumbling, its, Mrs, observers,\n",
      "Nearest to were: pole, Anyhow, rapidity, nightingale, trying, broadside, ticket, snored,\n",
      "Nearest to it: aphorism, supreme, icily, fire, obscure, results, Montespan, sunstroke,\n",
      "Nearest to into: Bayonne, long, structure, discourteous, Cistercian, Tin, sidewise, avenues,\n",
      "Average loss at step  2000 :  56.6723090274\n",
      "Average loss at step  4000 :  13.6454269724\n",
      "Average loss at step  6000 :  9.82264078081\n",
      "Average loss at step  8000 :  5.41615830719\n",
      "Average loss at step  10000 :  4.96517021716\n",
      "Nearest to .: ?, diddle, !, modest, :, ,, burned, abide,\n",
      "Nearest to one: damages, mid, Astounded, goals, halt, corn, emotions, diddle,\n",
      "Nearest to she: he, I, view, they, pink, rays, who, Indeed,\n",
      "Nearest to at: for, in, on, priceless, friendship, landlords, license, fruit,\n",
      "Nearest to your: his, my, the, prisons, than, stammered, deigned, another,\n",
      "Nearest to M: coachman, amused, Jackdaws, Mazarin, land, This, Valliere, pointed,\n",
      "Nearest to to: presentiments, feels, scarps, will, not, argue, extreme, in,\n",
      "Nearest to had: has, was, statements, could, bottom, have, diddle, comprehended,\n",
      "Nearest to What: Who, Do, Pastoria, My, thoroughly, Ah, flattering, oaths,\n",
      "Nearest to said: bracelets, diddle, reminds, bar, castle, branches, whose, Believe,\n",
      "Nearest to very: aware, received, endowed, movement, attentive, attentions, as, spectacula,\n",
      "Nearest to ,: diddle, senses, banks, bottom, contrast, ., lull, that,\n",
      "Nearest to was: is, had, Mrs, jetty, virtue, preposterous, priory, s,\n",
      "Nearest to were: rapidity, exist, pole, rang, had, broadside, soft, trying,\n",
      "Nearest to it: I, tender, him, submit, he, nothing, aphorism, Pray,\n",
      "Nearest to into: Tin, Bayonne, without, in, long, fools, pace, getting,\n",
      "Average loss at step  12000 :  4.86839444101\n",
      "Average loss at step  14000 :  4.34889706719\n",
      "Average loss at step  16000 :  4.38070407891\n",
      "Average loss at step  18000 :  4.29739814448\n",
      "Average loss at step  20000 :  4.17283791113\n",
      "Nearest to .: ?, !, :, ,, diddle, modest, victim, abide,\n",
      "Nearest to one: Astounded, coracles, mid, damages, corn, emotions, halt, Dennis,\n",
      "Nearest to she: he, I, they, who, it, Six, Indeed, bonfire,\n",
      "Nearest to at: for, on, stifled, landlords, in, friendship, priceless, charmingly,\n",
      "Nearest to your: his, my, their, prisons, her, the, begone, its,\n",
      "Nearest to M: coachman, Jackdaws, amused, This, Mazarin, land, ice, pointed,\n",
      "Nearest to to: presentiments, not, will, argue, scarps, feels, altered, unwell,\n",
      "Nearest to had: has, have, was, could, comprehended, burnt, statements, were,\n",
      "Nearest to What: Who, My, Pastoria, Ah, Do, Then, flattering, oaths,\n",
      "Nearest to said: replied, reminds, bracelets, diddle, bane, castle, talks, whose,\n",
      "Nearest to very: endowed, aware, as, attentions, attentive, movement, received, reappearing,\n",
      "Nearest to ,: diddle, imperious, introduce, aged, senses, ., descending, banks,\n",
      "Nearest to was: is, had, am, were, seemed, closes, anticipate, jetty,\n",
      "Nearest to were: exist, are, rapidity, was, rang, broadside, had, dread,\n",
      "Nearest to it: I, It, nothing, he, submit, him, Pray, well,\n",
      "Nearest to into: Tin, without, in, sidewise, Bayonne, fools, screen, to,\n",
      "Average loss at step  22000 :  4.25815713108\n",
      "Average loss at step  24000 :  4.12806034398\n",
      "Average loss at step  26000 :  4.09850939322\n",
      "Average loss at step  28000 :  4.1687569381\n",
      "Average loss at step  30000 :  4.05441103697\n",
      "Nearest to .: ?, !, :, ,, burned, million, diddle, abide,\n",
      "Nearest to one: Astounded, coracles, mid, damages, halt, emotions, Dennis, corn,\n",
      "Nearest to she: he, I, they, it, who, George, Six, bonfire,\n",
      "Nearest to at: stifled, landlords, priceless, shopman, simpering, friendship, pandemonium, Woods,\n",
      "Nearest to your: his, my, their, its, begone, her, prisons, songs,\n",
      "Nearest to M: coachman, Jackdaws, amused, ice, This, Mazarin, comprehended, Goring,\n",
      "Nearest to to: argue, against, presentiments, will, scarps, fretful, not, feels,\n",
      "Nearest to had: has, have, was, comprehended, statements, burnt, could, having,\n",
      "Nearest to What: Who, Ah, Pastoria, My, Do, It, Did, Then,\n",
      "Nearest to said: replied, cried, exclaimed, diddle, reminds, talks, sire, bracelets,\n",
      "Nearest to very: endowed, aware, as, attentions, attentive, movement, received, enormous,\n",
      "Nearest to ,: diddle, imperious, ., appreciate, proposed, lull, enable, accordance,\n",
      "Nearest to was: is, had, seemed, closes, am, anticipate, moored, preposterous,\n",
      "Nearest to were: are, exist, rang, rapidity, dread, was, broadside, nightingale,\n",
      "Nearest to it: nothing, I, he, It, Pray, submit, them, him,\n",
      "Nearest to into: Tin, without, sidewise, screen, empire, in, fools, Bayonne,\n",
      "Average loss at step  32000 :  4.03315892029\n",
      "Average loss at step  34000 :  4.07906727028\n",
      "Average loss at step  36000 :  4.01756280565\n",
      "Average loss at step  38000 :  4.00901840341\n",
      "Average loss at step  40000 :  4.00031196415\n",
      "Nearest to .: ?, :, abide, victim, !, burned, don, Tonnay,\n",
      "Nearest to one: Astounded, coracles, mid, Dennis, mood, halt, goals, Understand,\n",
      "Nearest to she: he, I, they, George, it, Finish, It, sawed,\n",
      "Nearest to at: stifled, landlords, priceless, pandemonium, shopman, simpering, collecting, nervously,\n",
      "Nearest to your: my, his, begone, prisons, songs, its, their, inscribe,\n",
      "Nearest to M: coachman, Jackdaws, amused, Hearing, ice, Mazarin, comprehended, tactics,\n",
      "Nearest to to: argue, presentiments, against, scarps, feels, altered, will, fretful,\n",
      "Nearest to had: has, have, comprehended, having, statements, burnt, accomplices, could,\n",
      "Nearest to What: Who, Ah, Pastoria, My, It, Did, Do, beverage,\n",
      "Nearest to said: replied, cried, exclaimed, added, returned, answered, sire, talks,\n",
      "Nearest to very: endowed, aware, attentive, as, attentions, enough, enormous, movement,\n",
      "Nearest to ,: senses, begged, diddle, introduce, lull, volant, editing, proposed,\n",
      "Nearest to was: is, am, seemed, closes, preposterous, anticipate, be, virtue,\n",
      "Nearest to were: are, exist, dread, rang, rapidity, precede, nightingale, broadside,\n",
      "Nearest to it: nothing, It, them, he, I, Athos, submit, Pray,\n",
      "Nearest to into: Tin, without, sidewise, empire, screen, obstacle, fools, Bayonne,\n",
      "Average loss at step  42000 :  3.96022699702\n",
      "Average loss at step  44000 :  3.99656808662\n",
      "Average loss at step  46000 :  3.9328572439\n",
      "Average loss at step  48000 :  3.9277932477\n",
      "Average loss at step  50000 :  3.9749354279\n",
      "Nearest to .: abide, ?, victim, bargain, Tonnay, arranging, domo, Richelieu,\n",
      "Nearest to one: Astounded, coracles, mid, Dennis, Understand, mood, jewel, halt,\n",
      "Nearest to she: he, I, they, George, Finish, resorted, Buckingham, It,\n",
      "Nearest to at: stifled, landlords, priceless, collecting, nervously, simpering, concerned, shopman,\n",
      "Nearest to your: my, his, begone, its, inscribe, songs, prisons, their,\n",
      "Nearest to M: coachman, Jackdaws, Hearing, amused, ice, heaven, And, cadaverous,\n",
      "Nearest to to: argue, presentiments, feels, scarps, altered, fretful, Title, against,\n",
      "Nearest to had: has, have, comprehended, statements, accomplices, having, burnt, shopman,\n",
      "Nearest to What: Who, Ah, My, It, Pastoria, Did, Do, beverage,\n",
      "Nearest to said: replied, cried, exclaimed, added, returned, answered, talks, gasped,\n",
      "Nearest to very: endowed, aware, attentive, as, enough, attentions, enormous, Very,\n",
      "Nearest to ,: introduce, lull, games, senses, hollows, writes, shrugged, contrast,\n",
      "Nearest to was: is, am, closes, seemed, be, preposterous, anticipate, per,\n",
      "Nearest to were: are, exist, rang, dread, rapidity, precede, nightingale, am,\n",
      "Nearest to it: nothing, It, submit, them, Athos, Listen, treason, Pray,\n",
      "Nearest to into: Tin, empire, sidewise, without, obstacle, screen, beings, fools,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  52000 :  3.87652115679\n",
      "Average loss at step  54000 :  3.89657744205\n",
      "Average loss at step  56000 :  3.92887654233\n",
      "Average loss at step  58000 :  3.85994894731\n",
      "Average loss at step  60000 :  3.86649897933\n",
      "Nearest to .: ?, bargain, abide, victim, carving, don, Richelieu, burned,\n",
      "Nearest to one: Astounded, coracles, mid, Dennis, Understand, jewel, One, UNDER,\n",
      "Nearest to she: he, I, they, George, resorted, Finish, Buckingham, sawed,\n",
      "Nearest to at: stifled, landlords, collecting, nervously, priceless, pandemonium, shrub, shopman,\n",
      "Nearest to your: my, his, inscribe, begone, songs, prisons, its, Your,\n",
      "Nearest to M: coachman, Hearing, Jackdaws, heaven, cadaverous, De, amused, And,\n",
      "Nearest to to: argue, presentiments, fretful, feels, scarps, Title, hesitate, will,\n",
      "Nearest to had: has, have, comprehended, having, statements, accomplices, Perhaps, burnt,\n",
      "Nearest to What: Ah, Who, Did, It, My, Do, Pastoria, beverage,\n",
      "Nearest to said: replied, cried, exclaimed, added, returned, answered, gasped, continued,\n",
      "Nearest to very: endowed, aware, attentive, enough, enormous, mixture, as, Very,\n",
      "Nearest to ,: descending, introduce, contrast, diddle, grim, imperious, discovered, treat,\n",
      "Nearest to was: is, am, closes, seemed, be, preposterous, anticipate, per,\n",
      "Nearest to were: are, exist, dread, rang, precede, nightingale, rapidity, am,\n",
      "Nearest to it: It, nothing, approbation, submit, Athos, I, he, Pray,\n",
      "Nearest to into: Tin, empire, sidewise, without, obstacle, swains, beings, screen,\n",
      "Average loss at step  62000 :  3.86430570817\n",
      "Average loss at step  64000 :  3.8536888715\n",
      "Average loss at step  66000 :  3.85675087118\n",
      "Average loss at step  68000 :  3.82673781061\n",
      "Average loss at step  70000 :  3.82375520134\n",
      "Nearest to .: ?, don, abide, ,, chlamys, Tonnay, aground, !,\n",
      "Nearest to one: Astounded, coracles, Understand, mid, Dennis, Alfred, UNDER, One,\n",
      "Nearest to she: he, I, they, George, resorted, Buckingham, Finish, It,\n",
      "Nearest to at: stifled, landlords, shrub, nervously, collecting, concerned, pandemonium, priceless,\n",
      "Nearest to your: my, his, inscribe, Your, begone, prisons, songs, volant,\n",
      "Nearest to M: coachman, Hearing, heaven, cadaverous, De, tactics, Jackdaws, And,\n",
      "Nearest to to: argue, feels, presentiments, scarps, fretful, Title, unwell, not,\n",
      "Nearest to had: has, have, comprehended, having, accomplices, Perhaps, statements, ve,\n",
      "Nearest to What: Ah, Who, It, Did, beverage, Do, My, Pastoria,\n",
      "Nearest to said: replied, cried, exclaimed, added, answered, returned, gasped, continued,\n",
      "Nearest to very: endowed, aware, enough, attentive, enormous, mixture, withdraw, Very,\n",
      "Nearest to ,: aged, imperious, proposed, introduce, accordance, appreciate, enable, neuralgia,\n",
      "Nearest to was: is, am, closes, seemed, be, preposterous, priory, per,\n",
      "Nearest to were: are, exist, dread, precede, rang, nightingale, am, rapidity,\n",
      "Nearest to it: nothing, It, approbation, I, Listen, Understand, Pray, submit,\n",
      "Nearest to into: Tin, empire, sidewise, swains, without, obstacle, beings, discourteous,\n",
      "Average loss at step  72000 :  3.85297926927\n",
      "Average loss at step  74000 :  3.77829272449\n",
      "Average loss at step  76000 :  3.80533335853\n",
      "Average loss at step  78000 :  3.82668840933\n",
      "Average loss at step  80000 :  3.75533073246\n",
      "Nearest to .: ?, don, Tonnay, chlamys, hate, ,, actively, pleases,\n",
      "Nearest to one: Astounded, coracles, One, Dennis, UNDER, Understand, mid, Alfred,\n",
      "Nearest to she: he, I, they, resorted, George, Buckingham, sawed, Finish,\n",
      "Nearest to at: stifled, collecting, landlords, nervously, shrub, pandemonium, Joints, shopman,\n",
      "Nearest to your: my, his, inscribe, songs, Your, prisons, begone, volant,\n",
      "Nearest to M: coachman, Hearing, De, heaven, cadaverous, Bodily, each, tactics,\n",
      "Nearest to to: argue, presentiments, fretful, scarps, Title, feels, To, harbor,\n",
      "Nearest to had: has, have, comprehended, accomplices, Perhaps, statements, ve, having,\n",
      "Nearest to What: Ah, Who, Did, It, beverage, Do, cells, My,\n",
      "Nearest to said: replied, cried, exclaimed, added, answered, returned, gasped, continued,\n",
      "Nearest to very: endowed, aware, enough, enormous, attentive, Very, withdraw, mixture,\n",
      "Nearest to ,: appreciate, hindering, lull, owe, begged, dumb, proposed, junior,\n",
      "Nearest to was: is, am, closes, seemed, be, preposterous, priory, anticipate,\n",
      "Nearest to were: are, exist, nightingale, precede, dread, rang, am, Anyhow,\n",
      "Nearest to it: Listen, approbation, It, random, Pray, nothing, simply, submit,\n",
      "Nearest to into: Tin, empire, sidewise, swains, without, obstacle, beings, discourteous,\n",
      "Average loss at step  82000 :  3.78701607394\n",
      "Average loss at step  84000 :  3.78501872611\n",
      "Average loss at step  86000 :  3.7552367599\n",
      "Average loss at step  88000 :  3.77004337418\n",
      "Average loss at step  90000 :  3.7359818151\n",
      "Nearest to .: abide, victim, don, ?, AS, Tonnay, hate, bargain,\n",
      "Nearest to one: Astounded, coracles, Dennis, One, Graves, UNDER, Alfred, Understand,\n",
      "Nearest to she: he, I, they, resorted, George, Buckingham, sawed, Finish,\n",
      "Nearest to at: stifled, landlords, collecting, nervously, shrub, Joints, tidied, priceless,\n",
      "Nearest to your: my, his, inscribe, songs, Your, prisons, begone, deigned,\n",
      "Nearest to M: coachman, Hearing, heaven, De, cadaverous, tactics, each, Bodily,\n",
      "Nearest to to: argue, presentiments, fretful, scarps, Title, repacked, To, feels,\n",
      "Nearest to had: has, have, comprehended, Perhaps, ve, accomplices, statements, solicitude,\n",
      "Nearest to What: Did, Ah, Who, It, beverage, Do, cells, My,\n",
      "Nearest to said: replied, cried, added, exclaimed, answered, returned, gasped, continued,\n",
      "Nearest to very: endowed, aware, enough, enormous, Very, withdraw, attentive, coldest,\n",
      "Nearest to ,: introduce, senses, lull, warningly, lameness, hollows, haven, deserve,\n",
      "Nearest to was: is, am, closes, be, preposterous, seemed, priory, anticipate,\n",
      "Nearest to were: are, exist, nightingale, precede, dread, am, rang, Anyhow,\n",
      "Nearest to it: Listen, It, approbation, random, treason, simply, Pray, submit,\n",
      "Nearest to into: empire, Tin, sidewise, swains, obstacle, beings, discourteous, without,\n",
      "Average loss at step  92000 :  3.75659408092\n",
      "Average loss at step  94000 :  3.76567441475\n",
      "Average loss at step  96000 :  3.71380198091\n",
      "Average loss at step  98000 :  3.73776411891\n",
      "Average loss at step  100000 :  3.75455930984\n",
      "Nearest to .: abide, fortunately, XIV, AS, Richelieu, woke, yards, plunging,\n",
      "Nearest to one: Astounded, coracles, Graves, One, endeavouring, Dennis, Alfred, Understand,\n",
      "Nearest to she: he, I, they, resorted, Finish, George, Buckingham, sawed,\n",
      "Nearest to at: stifled, landlords, nervously, collecting, shrub, Joints, hurries, pandemonium,\n",
      "Nearest to your: my, his, inscribe, Your, songs, prisons, volant, begone,\n",
      "Nearest to M: Hearing, coachman, De, heaven, cadaverous, each, tactics, Bodily,\n",
      "Nearest to to: argue, scarps, presentiments, harbor, To, fretful, Title, treat,\n",
      "Nearest to had: has, have, comprehended, Perhaps, ve, accomplices, statements, having,\n",
      "Nearest to What: Ah, Did, Who, beverage, Do, It, cells, My,\n",
      "Nearest to said: replied, cried, added, exclaimed, returned, gasped, continued, answered,\n",
      "Nearest to very: endowed, aware, enough, enormous, withdraw, Very, attentive, coldest,\n",
      "Nearest to ,: moonlit, descending, congenial, sneaked, decide, bottom, shrugged, lull,\n",
      "Nearest to was: is, am, closes, be, preposterous, per, characterized, seemed,\n",
      "Nearest to were: are, exist, nightingale, precede, am, dread, Anyhow, painter,\n",
      "Nearest to it: Listen, approbation, random, treason, It, simply, sunstroke, Aramis,\n",
      "Nearest to into: empire, Tin, sidewise, swains, beings, discourteous, without, affiliated,\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "embedding_size = 128  # Dimension of the embedding vector.\n",
    "skip_window = 1       # How many words to consider left and right.\n",
    "num_skips = 2         # How many times to reuse an input to generate a label.\n",
    "num_sampled = 64      # Number of negative examples to sample.\n",
    "\n",
    "# We pick a random validation set to sample nearest neighbors. Here we limit the\n",
    "# validation samples to the words that have a low numeric ID, which by\n",
    "# construction are also the most frequent. These 3 variables are used only for\n",
    "# displaying model accuracy, they don't affect calculation.\n",
    "valid_size = 16     # Random set of words to evaluate similarity on.\n",
    "valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data.\n",
    "    train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "\n",
    "    # Ops and variables pinned to the CPU because of missing GPU implementation\n",
    "    # Look up embeddings for inputs.\n",
    "    embeddings = tf.Variable(\n",
    "        tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "\n",
    "    # Construct the variables for the NCE loss\n",
    "    nce_weights = tf.Variable(\n",
    "        tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                            stddev=1.0 / math.sqrt(embedding_size)))\n",
    "    nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "    # Compute the average NCE loss for the batch.\n",
    "    # tf.nce_loss automatically draws a new sample of the negative labels each\n",
    "    # time we evaluate the loss.\n",
    "    # Explanation of the meaning of NCE loss:\n",
    "    #   http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.nce_loss(weights=nce_weights,\n",
    "                     biases=nce_biases,\n",
    "                     labels=train_labels,\n",
    "                     inputs=embed,\n",
    "                     num_sampled=num_sampled,\n",
    "                     num_classes=vocabulary_size))\n",
    "    # Construct the SGD optimizer using a learning rate of 1.0.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n",
    "\n",
    "    # Compute the cosine similarity between minibatch examples and all embeddings.\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    valid_embeddings = tf.nn.embedding_lookup(\n",
    "      normalized_embeddings, valid_dataset)\n",
    "    similarity = tf.matmul(\n",
    "      valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "\n",
    "    # Add variable initializer.\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "# Step 5: Begin training.\n",
    "num_steps = 100001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    # We must initialize all variables before we use them.\n",
    "    init.run()\n",
    "    print('Initialized')\n",
    "\n",
    "    average_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batch_inputs, batch_labels = generate_batch(\n",
    "            batch_size, num_skips, skip_window)\n",
    "        feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}\n",
    "\n",
    "        # We perform one update step by evaluating the optimizer op (including it\n",
    "        # in the list of returned values for session.run()\n",
    "        _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "        average_loss += loss_val\n",
    "\n",
    "        if step % 2000 == 0:\n",
    "            if step > 0:\n",
    "                average_loss /= 2000\n",
    "            # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "            print('Average loss at step ', step, ': ', average_loss)\n",
    "            average_loss = 0\n",
    "\n",
    "        # Note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "        if step % 10000 == 0:\n",
    "            sim = similarity.eval()\n",
    "            for i in range(valid_size):\n",
    "                valid_word = reverse_dictionary[valid_examples[i]]\n",
    "                top_k = 8  # number of nearest neighbors\n",
    "                nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "                log_str = 'Nearest to %s:' % valid_word\n",
    "                for k in range(top_k):\n",
    "                    close_word = reverse_dictionary[nearest[k]]\n",
    "                    log_str = '%s %s,' % (log_str, close_word)\n",
    "                print(log_str)\n",
    "    final_embeddings = normalized_embeddings.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pylint: disable=missing-docstring\n",
    "# Function to draw visualization of distance between embeddings.\n",
    "def plot_with_labels(low_dim_embs, labels, filename):\n",
    "    assert low_dim_embs.shape[0] >= len(labels), 'More labels than embeddings'\n",
    "    plt.figure(figsize=(18, 18))  # in inches\n",
    "    for i, label in enumerate(labels):\n",
    "        x, y = low_dim_embs[i, :]\n",
    "        plt.scatter(x, y)\n",
    "        plt.annotate(label,\n",
    "                 xy=(x, y),\n",
    "                 xytext=(5, 2),\n",
    "                 textcoords='offset points',\n",
    "                 ha='right',\n",
    "                 va='bottom')\n",
    "\n",
    "    plt.savefig(filename)\n",
    "\n",
    "try:\n",
    "    # pylint: disable=g-import-not-at-top\n",
    "    from sklearn.manifold import TSNE\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000, method='exact')\n",
    "    plot_only = 500\n",
    "    low_dim_embs = tsne.fit_transform(final_embeddings[:plot_only, :])\n",
    "    labels = [reverse_dictionary[i] for i in range(plot_only)]\n",
    "    plot_with_labels(low_dim_embs, labels, os.path.join('d:\\\\', 'tsne.png'))\n",
    "\n",
    "except ImportError as ex:\n",
    "    print('Please install sklearn, matplotlib, and scipy to show embeddings.')\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
